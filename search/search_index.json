{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RNAsamba Overview RNAsamba is a tool for computing the coding potential of RNA sequences using a neural network classification model. It can be used to identify mRNAs and lncRNAs without relying on database searches. For details of RNAsamba's methods, check the Theory section or read our article . Source code RNAsamba is an open source package distributed under the GPL-3.0 license. Its source code can be found in the GitHub repository . Installation There are two ways to install RNAsamba: Using pip: pip install rnasamba Using conda: conda install - c bioconda rnasamba If you want to use RNAsamba with a GPU, you first need to install tensorflow-gpu>=1.5.0,<2.0 , keras>=2.1.0 , numpy and bioconda with pip and then proceed to install rnasamba using the --no-deps argument. Citation Camargo, Antonio P., Vsevolod Sourkov, and Marcelo F. Carazzolle. \" RNAsamba: coding potential assessment using ORF and whole transcript sequence information \" BioRxiv (2019).","title":"Home"},{"location":"#rnasamba","text":"","title":"RNAsamba"},{"location":"#overview","text":"RNAsamba is a tool for computing the coding potential of RNA sequences using a neural network classification model. It can be used to identify mRNAs and lncRNAs without relying on database searches. For details of RNAsamba's methods, check the Theory section or read our article .","title":"Overview"},{"location":"#source-code","text":"RNAsamba is an open source package distributed under the GPL-3.0 license. Its source code can be found in the GitHub repository .","title":"Source code"},{"location":"#installation","text":"There are two ways to install RNAsamba: Using pip: pip install rnasamba Using conda: conda install - c bioconda rnasamba If you want to use RNAsamba with a GPU, you first need to install tensorflow-gpu>=1.5.0,<2.0 , keras>=2.1.0 , numpy and bioconda with pip and then proceed to install rnasamba using the --no-deps argument.","title":"Installation"},{"location":"#citation","text":"Camargo, Antonio P., Vsevolod Sourkov, and Marcelo F. Carazzolle. \" RNAsamba: coding potential assessment using ORF and whole transcript sequence information \" BioRxiv (2019).","title":"Citation"},{"location":"theory/","text":"Theory IGLOO sequence representations Traditionally, recurrent neural networks (RNNs) are the go-to models to create high level representations from sequence data. However, these networks don't perform well with long sequences and their training can be very slow. To achieve better classification performance and speed, RNAsamba uses the recently introduced IGLOO architecture 1 looks at sequences as a whole, rather than sequentially like in the recurrent paradigm. To do so, IGLOO creates representations of sequences by taking patches of the feature space and multiplying them by learnable weights. By taking global snapshots of the sequence, IGLOO networks can be used to process very long sequences, making them particularly interesting for nucleotide sequence data. Two branch structure Starting from the initial nucleotide sequence, RNAsamba computes the coding potential of a given transcript by combining information coming from two different sources: the Whole Sequence Branch (B 1 ) and the Longest ORF Branch (B 2 ). B 1 contains whole sequence representations of the transcript and can capture protein-coding signatures irrespective of the identification of the ORF. In contrast, B 2 carries information extracted from the longest identified ORF and the putative protein translated from it. By taking into account these two sources of sequence information, RNAsamba builds a thorough model of the transcript, improving the classification performance of the algorithm. For a more detailed description of RNAsamba's algorithm, please check our article . Sourkov, Vsevolod. \"IGLOO: Slicing the Features Space to Represent Long Sequences.\" arXiv (2018). \u21a9","title":"Theory"},{"location":"theory/#theory","text":"","title":"Theory"},{"location":"theory/#igloo-sequence-representations","text":"Traditionally, recurrent neural networks (RNNs) are the go-to models to create high level representations from sequence data. However, these networks don't perform well with long sequences and their training can be very slow. To achieve better classification performance and speed, RNAsamba uses the recently introduced IGLOO architecture 1 looks at sequences as a whole, rather than sequentially like in the recurrent paradigm. To do so, IGLOO creates representations of sequences by taking patches of the feature space and multiplying them by learnable weights. By taking global snapshots of the sequence, IGLOO networks can be used to process very long sequences, making them particularly interesting for nucleotide sequence data.","title":"IGLOO sequence representations"},{"location":"theory/#two-branch-structure","text":"Starting from the initial nucleotide sequence, RNAsamba computes the coding potential of a given transcript by combining information coming from two different sources: the Whole Sequence Branch (B 1 ) and the Longest ORF Branch (B 2 ). B 1 contains whole sequence representations of the transcript and can capture protein-coding signatures irrespective of the identification of the ORF. In contrast, B 2 carries information extracted from the longest identified ORF and the putative protein translated from it. By taking into account these two sources of sequence information, RNAsamba builds a thorough model of the transcript, improving the classification performance of the algorithm. For a more detailed description of RNAsamba's algorithm, please check our article . Sourkov, Vsevolod. \"IGLOO: Slicing the Features Space to Represent Long Sequences.\" arXiv (2018). \u21a9","title":"Two branch structure"},{"location":"usage/","text":"Usage Download the pre-trained model We provide a HDF5 file containing the weights of a classification model trained with human trancript sequences. This model achieves high classification performance even in transcripts of distant species (see our article ). In case you want to train your own model, you should follow the steps described in the rnasamba-train section. You can download the weights file by executing the following command: curl - O https : // raw . githubusercontent . com / apcamargo / RNAsamba / master / data / weights_master_model . hdf5 rnasamba-train rnasamba-train is the command for training a new classification model from a training dataset and saving the network weights into an HDF5 file. The user can specify the batch size ( --batch_size ) and the number of training epochs ( --epochs ). The user can also choose to activate early stopping ( --early_stopping ), which reduces training time and can help avoiding overfitting. usage : rnasamba - train [ - h ] [ - s EARLY_STOPPING ] [ - b BATCH_SIZE ] [ - e EPOCHS ] [ - v { 0 , 1 , 2 , 3 }] output_file coding_file noncoding_file Train a new classification model . positional arguments : output_file output HDF5 file containing weights of the newly trained RNAsamba network . coding_file input FASTA file containing sequences of protein - coding transcripts . noncoding_file input FASTA file containing sequences of noncoding transcripts . optional arguments : - h , -- help show this help message and exit - s EARLY_STOPPING , -- early_stopping EARLY_STOPPING number of epochs after lowest validation loss before stopping training ( a fraction of 0 . 1 of the training set is set apart for validation and the model with the lowest validation loss will be saved ) . ( default : 0 ) - b BATCH_SIZE , -- batch_size BATCH_SIZE number of samples per gradient update . ( default : 128 ) - e EPOCHS , -- epochs EPOCHS number of epochs to train the model . ( default : 40 ) - v { 0 , 1 , 2 , 3 }, -- verbose { 0 , 1 , 2 , 3 } print the progress of the training . 0 = silent , 1 = current step , 2 = progress bar , 3 = one line per epoch . ( default : 0 ) rnasamba-classify rnasamba-classify is the command for computing the coding potential of transcripts contained in an input FASTA file and classifying them into coding or non-coding. Optionally, the user can specify an output FASTA file ( --protein_fasta ) in which RNAsamba will write the translated sequences of the predicted coding ORFs. If multiple weight files are provided, RNAsamba will ensemble their predictions into a single output. usage : rnasamba - classify [ - h ] [ - p PROTEIN_FASTA ] [ - v { 0 , 1 }] output_file fasta_file weights [ weights ...] Classify sequences from a input FASTA file . positional arguments : output_file output TSV file containing the results of the classification . fasta_file input FASTA file containing transcript sequences . weights input HDF5 file ( s ) containing weights of a trained RNAsamba network ( if more than a file is provided , an ensembling of the models will be performed ) . optional arguments : - h , -- help show this help message and exit - p PROTEIN_FASTA , -- protein_fasta PROTEIN_FASTA output FASTA file containing translated sequences for the predicted coding ORFs . ( default : None ) - v { 0 , 1 }, -- verbose { 0 , 1 } print the progress of the classification . 0 = silent , 1 = current step . ( default : 0 ) Examples Training a new classification model using Mus musculus data downloaded from GENCODE: rnasamba - train mouse_model . hdf5 - v 2 gencode . vM21 . pc_transcripts . fa gencode . vM21 . lncRNA_transcripts . fa Classifying sequences using our pre-trained model ( weights_master_model.hdf5 ) and saving the predicted proteins into a FASTA file: rnasamba - classify - p predicted_proteins . fa classification . tsv input . fa weights_master_model . hdf5 head classification . tsv sequence_name coding_score classification ENSMUST00000054910 0 . 99022 coding ENSMUST00000059648 0 . 84718 coding ENSMUST00000055537 0 . 99713 coding ENSMUST00000030975 0 . 85189 coding ENSMUST00000050754 0 . 02638 noncoding ENSMUST00000008011 0 . 14949 noncoding ENSMUST00000061643 0 . 03456 noncoding ENSMUST00000059704 0 . 89232 coding ENSMUST00000036304 0 . 03782 noncoding","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#download-the-pre-trained-model","text":"We provide a HDF5 file containing the weights of a classification model trained with human trancript sequences. This model achieves high classification performance even in transcripts of distant species (see our article ). In case you want to train your own model, you should follow the steps described in the rnasamba-train section. You can download the weights file by executing the following command: curl - O https : // raw . githubusercontent . com / apcamargo / RNAsamba / master / data / weights_master_model . hdf5","title":"Download the pre-trained model"},{"location":"usage/#rnasamba-train","text":"rnasamba-train is the command for training a new classification model from a training dataset and saving the network weights into an HDF5 file. The user can specify the batch size ( --batch_size ) and the number of training epochs ( --epochs ). The user can also choose to activate early stopping ( --early_stopping ), which reduces training time and can help avoiding overfitting. usage : rnasamba - train [ - h ] [ - s EARLY_STOPPING ] [ - b BATCH_SIZE ] [ - e EPOCHS ] [ - v { 0 , 1 , 2 , 3 }] output_file coding_file noncoding_file Train a new classification model . positional arguments : output_file output HDF5 file containing weights of the newly trained RNAsamba network . coding_file input FASTA file containing sequences of protein - coding transcripts . noncoding_file input FASTA file containing sequences of noncoding transcripts . optional arguments : - h , -- help show this help message and exit - s EARLY_STOPPING , -- early_stopping EARLY_STOPPING number of epochs after lowest validation loss before stopping training ( a fraction of 0 . 1 of the training set is set apart for validation and the model with the lowest validation loss will be saved ) . ( default : 0 ) - b BATCH_SIZE , -- batch_size BATCH_SIZE number of samples per gradient update . ( default : 128 ) - e EPOCHS , -- epochs EPOCHS number of epochs to train the model . ( default : 40 ) - v { 0 , 1 , 2 , 3 }, -- verbose { 0 , 1 , 2 , 3 } print the progress of the training . 0 = silent , 1 = current step , 2 = progress bar , 3 = one line per epoch . ( default : 0 )","title":"rnasamba-train"},{"location":"usage/#rnasamba-classify","text":"rnasamba-classify is the command for computing the coding potential of transcripts contained in an input FASTA file and classifying them into coding or non-coding. Optionally, the user can specify an output FASTA file ( --protein_fasta ) in which RNAsamba will write the translated sequences of the predicted coding ORFs. If multiple weight files are provided, RNAsamba will ensemble their predictions into a single output. usage : rnasamba - classify [ - h ] [ - p PROTEIN_FASTA ] [ - v { 0 , 1 }] output_file fasta_file weights [ weights ...] Classify sequences from a input FASTA file . positional arguments : output_file output TSV file containing the results of the classification . fasta_file input FASTA file containing transcript sequences . weights input HDF5 file ( s ) containing weights of a trained RNAsamba network ( if more than a file is provided , an ensembling of the models will be performed ) . optional arguments : - h , -- help show this help message and exit - p PROTEIN_FASTA , -- protein_fasta PROTEIN_FASTA output FASTA file containing translated sequences for the predicted coding ORFs . ( default : None ) - v { 0 , 1 }, -- verbose { 0 , 1 } print the progress of the classification . 0 = silent , 1 = current step . ( default : 0 )","title":"rnasamba-classify"},{"location":"usage/#examples","text":"Training a new classification model using Mus musculus data downloaded from GENCODE: rnasamba - train mouse_model . hdf5 - v 2 gencode . vM21 . pc_transcripts . fa gencode . vM21 . lncRNA_transcripts . fa Classifying sequences using our pre-trained model ( weights_master_model.hdf5 ) and saving the predicted proteins into a FASTA file: rnasamba - classify - p predicted_proteins . fa classification . tsv input . fa weights_master_model . hdf5 head classification . tsv sequence_name coding_score classification ENSMUST00000054910 0 . 99022 coding ENSMUST00000059648 0 . 84718 coding ENSMUST00000055537 0 . 99713 coding ENSMUST00000030975 0 . 85189 coding ENSMUST00000050754 0 . 02638 noncoding ENSMUST00000008011 0 . 14949 noncoding ENSMUST00000061643 0 . 03456 noncoding ENSMUST00000059704 0 . 89232 coding ENSMUST00000036304 0 . 03782 noncoding","title":"Examples"}]}